{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9757e00d",
   "metadata": {},
   "source": [
    "# Water Potability Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1ca005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/work')\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.sql import SparkSession, Window, DataFrame\n",
    "from utilities.minio_utils import  log_artifacts_minio\n",
    "from pipeline import feature_pipeline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb42188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "ml.combust.mleap#mleap-spark-base_2.11 added as a dependency\n",
      "ml.combust.mleap#mleap-spark_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1c246194-c63f-420e-b863-c7e4d484c6a3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound ml.combust.mleap#mleap-spark-base_2.11;0.16.0 in central\n",
      "\tfound ml.combust.mleap#mleap-runtime_2.11;0.16.0 in central\n",
      "\tfound ml.combust.mleap#mleap-core_2.11;0.16.0 in central\n",
      "\tfound ml.combust.mleap#mleap-base_2.11;0.16.0 in central\n",
      "\tfound ml.combust.mleap#mleap-tensor_2.11;0.16.0 in central\n",
      "\tfound io.spray#spray-json_2.11;1.3.2 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound ml.combust.bundle#bundle-ml_2.11;0.16.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.5.1 in central\n",
      "\tfound com.thesamet.scalapb#scalapb-runtime_2.11;0.7.1 in central\n",
      "\tfound com.thesamet.scalapb#lenses_2.11;0.7.0-test2 in central\n",
      "\tfound com.lihaoyi#fastparse_2.11;1.0.0 in central\n",
      "\tfound com.lihaoyi#fastparse-utils_2.11;1.0.0 in central\n",
      "\tfound com.lihaoyi#sourcecode_2.11;0.1.4 in central\n",
      "\tfound com.jsuereth#scala-arm_2.11;2.0 in central\n",
      "\tfound com.typesafe#config;1.3.0 in central\n",
      "\tfound commons-io#commons-io;2.5 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.12 in central\n",
      "\tfound ml.combust.bundle#bundle-hdfs_2.11;0.16.0 in central\n",
      "\tfound ml.combust.mleap#mleap-spark_2.11;0.16.0 in central\n",
      ":: resolution report :: resolve 741ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.5.1 from central in [default]\n",
      "\tcom.jsuereth#scala-arm_2.11;2.0 from central in [default]\n",
      "\tcom.lihaoyi#fastparse-utils_2.11;1.0.0 from central in [default]\n",
      "\tcom.lihaoyi#fastparse_2.11;1.0.0 from central in [default]\n",
      "\tcom.lihaoyi#sourcecode_2.11;0.1.4 from central in [default]\n",
      "\tcom.thesamet.scalapb#lenses_2.11;0.7.0-test2 from central in [default]\n",
      "\tcom.thesamet.scalapb#scalapb-runtime_2.11;0.7.1 from central in [default]\n",
      "\tcom.typesafe#config;1.3.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.5 from central in [default]\n",
      "\tio.spray#spray-json_2.11;1.3.2 from central in [default]\n",
      "\tml.combust.bundle#bundle-hdfs_2.11;0.16.0 from central in [default]\n",
      "\tml.combust.bundle#bundle-ml_2.11;0.16.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-base_2.11;0.16.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-core_2.11;0.16.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-runtime_2.11;0.16.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-spark-base_2.11;0.16.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-spark_2.11;0.16.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-tensor_2.11;0.16.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.12 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java;3.5.0 by [com.google.protobuf#protobuf-java;3.5.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   1   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1c246194-c63f-420e-b863-c7e4d484c6a3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 20 already retrieved (0kB/19ms)\n",
      "21/07/12 18:53:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .config('spark.jars.packages', 'ml.combust.mleap:mleap-spark-base_2.11:0.16.0,ml.combust.mleap:mleap-spark_2.11:0.16.0')\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde46dc",
   "metadata": {},
   "source": [
    "## Load the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585a2ce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+----------+\n",
      "|_c0|  ph|          Hardness|            Solids|       Chloramines|           Sulfate|      Conductivity|    Organic_carbon|  Trihalomethanes|         Turbidity|Potability|\n",
      "+---+----+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+----------+\n",
      "|  0|null|  98.3679148956603| 28415.57583214058|10.558949998467961|  296.843207792478|505.24026927891407|12.882614472289333|85.32995534051292| 4.119087300328971|         1|\n",
      "|  1|null|103.46475866009455| 27420.16742458204| 8.417305032089528|              null|485.97450045781375|11.351132730708514| 67.8699636759021| 4.620793451653219|         0|\n",
      "|  2|null|108.91662923953173|14476.335695268315| 5.398162017711099|  281.198274407849| 512.2323064106689|15.013793389990155| 86.6714587149138| 3.895572062268123|         1|\n",
      "|  3|null|113.17596460727073|  9943.92978526269| 6.337137942441213|354.29756524708256| 415.3383368798727| 19.67616854859483|23.07580599653685| 3.787475537347365|         1|\n",
      "|  4|null| 114.7335449715346| 13677.99404000127| 9.981200455815905|441.82677662870003|  524.000355172102|11.384858471731945|71.15328465919002|3.2938483740192734|         1|\n",
      "+---+----+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/water_potability_train.csv'\n",
    "df = spark.read.csv(file_path,inferSchema=True, header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23854c04",
   "metadata": {},
   "source": [
    "## Model Experimentation with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ece521",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'water_potability'\n",
    "mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c7a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "feature_cols = ['ph','Hardness','Solids','Chloramines','Sulfate','Conductivity',\n",
    "               'Organic_carbon', 'Trihalomethanes', 'Turbidity']\n",
    "assembler_out_col = 'Features'\n",
    "scaler_out_col = 'ScaledFeatures'\n",
    "expander_out_col = 'ExpandedFeatures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c1eb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mleap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1393/3640455266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                     scaler_out_col, expander_out_col, degree)\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Save the model pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     mlflow.spark.save_model(pipeline,\n\u001b[0m\u001b[1;32m      7\u001b[0m                             \u001b[0;34m'../models/pipeline_model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                             sample_input=df.select(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/mlflow/spark.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(spark_model, path, mlflow_model, conda_env, dfs_tmpdir, sample_input, signature, input_example)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0m_HadoopFileSystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_to_local_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparkml_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_src\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m     _save_model_metadata(\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0mdst_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mspark_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/mlflow/spark.py\u001b[0m in \u001b[0;36m_save_model_metadata\u001b[0;34m(dst_dir, spark_model, mlflow_model, sample_input, conda_env, signature, input_example)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_input\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         mleap.add_to_model(\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mmlflow_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlflow_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/mlflow/utils/annotations.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s only takes keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mnotice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\".. Note:: This method requires all argument be specified by keyword.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/mlflow/mleap.py\u001b[0m in \u001b[0;36madd_to_model\u001b[0;34m(mlflow_model, path, spark_model, sample_input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipelineModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmleap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mmleap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_support\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleSparkSerializer\u001b[0m  \u001b[0;31m# pylint: disable=unused-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mleap'"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    mlflow.log_param(\"degree\", degree)\n",
    "    pipeline = feature_pipeline.create_feature_pipeline(df, feature_cols, assembler_out_col, \n",
    "                                                    scaler_out_col, expander_out_col, degree)\n",
    "    # Save the model pipeline\n",
    "    mlflow.spark.save_model(pipeline,\n",
    "                            '../models/pipeline_model',\n",
    "                            sample_input=df.select(\n",
    "                                'ph','Hardness','Solids','Chloramines','Sulfate',\n",
    "                                'Conductivity', 'Organic_carbon', 'Trihalomethanes',\n",
    "                                'Turbidity'\n",
    "                            )\n",
    "    )\n",
    "    #log_artifacts_minio(run, '../models/pipeline_model', 'pipeline_mode', True)\n",
    "    run_id = run.info.run_id\n",
    "    print(run.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bf149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92de34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
